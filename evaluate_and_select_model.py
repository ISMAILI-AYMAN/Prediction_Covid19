# -*- coding: utf-8 -*-

"""
Module d'√©valuation et de s√©lection des mod√®les Markov
======================================================

Fournit des fonctions pour :
- Calculer l'erreur d'un mod√®le (MAE)
- √âvaluer la performance d'une matrice de transition sur les donn√©es
- Chercher le meilleur hyperparam√®tre (alpha g√©ographique)
- G√©n√©rer les mod√®les pond√©r√©s (LS/EM) et s√©lectionner le meilleur

Auteur : Harry FRANCIS (2025)
Version : 1.0.0
Compatibilit√© : Python 3.9+
"""


# --- Librairies standards
import logging
import os
from typing import Any

# --- Librairies tiers
import numpy as np
from tqdm import tqdm

# --- Modules locaux
from utils_io import sauvegarder_json
from utils_log import log_debut_fin_logger_dynamique
from markov_models import (
    appliquer_des_contraintes_geographiques, esperance_maximisation_markov_avec_geo, 
    estimer_matrice_de_transition_par_moindre_carre, 
    forcer_valeurs_positives_et_normaliser
)
POIDS_MOINDRE_CARRE = "poids_LS"
POIS_ESPERANCE_MAXIMISATION = "poids_EM"
MAE_VALIDATION = "mae_validation"
TRANSITION_MATRIX = "matrice_de_transition"
ALPHA_GEO = "alpha_geo"
BEST_ALPHA_GEO = "meilleur_alpha_geo"
BEST_MAE = "meilleur_mae"
NBR_OBSERVATION = "nbr_observation"
METADATA = "metadata"
NOM_DU_FICHIER = "nom_du_fichier"
NOMBRE_DE_TESTS_SOUHAITES = 400  # ou ce que tu veux


def calculer_mae(predictions: np.ndarray, cibles: np.ndarray) -> float:
    """
    Calcule l‚Äôerreur absolue moyenne (MAE) entre deux matrices ou vecteurs.

    Args:
        predictions (np.ndarray): Valeurs pr√©dites par un mod√®le ou une matrice.
        cibles (np.ndarray): Valeurs r√©elles de r√©f√©rence √† comparer.

    Returns:
        float: MAE (erreur absolue moyenne) entre predictions et cibles.

    Note:
        - Utilise la formule standard de la MAE‚ÄØ:
          moyenne des valeurs absolues des √©carts.
        - Fonctionne avec n‚Äôimporte quelle forme compatible de ndarray.
        - La MAE est toujours positive ou nulle‚ÄØ; plus elle est faible, 
          meilleure est la pr√©diction.

    Example:
        >>> y_pred = np.array([1.1, 2.0, 3.4])
        >>> y_true = np.array([1.0, 2.5, 3.0])
        >>> calculer_mae(y_pred, y_true)
        0.333333...

    √âtapes:
        1. Calcule l‚Äô√©cart absolu √©l√©ment par √©l√©ment.
        2. Calcule la moyenne de tous les √©carts.

    Tips:
        - Id√©al pour comparer des mod√®les ou ajuster des hyperparam√®tres.

    Utilisation:
        √Ä appeler √† chaque √©tape d‚Äô√©valuation pour comparer objectivement deux 
        ensembles de donn√©es num√©riques.

    Limitation:
        - Suppose que les tableaux sont de m√™me dimension.

    See also:
        - np.mean, np.abs
    """
    return float(np.mean(np.abs(predictions - cibles)))


def evaluer_matrice_transition(
        matrice_transition: np.ndarray, X_t: np.ndarray, X_t1: np.ndarray) -> float:
    """
    √âvalue la qualit√© d‚Äôune matrice de transition Markov sur des donn√©es COVID.

    Args:
        matrice_transition (np.ndarray): Matrice de transition (Markov, LS, EM‚Ä¶).
        X_t (np.ndarray): Matrice des cas au temps t (avant transition).
        X_t1 (np.ndarray): Matrice des cas au temps t+1 (apr√®s transition).

    Returns:
        float: Erreur MAE entre les valeurs pr√©dites et les vraies valeurs (X_t1).

    Note:
        - Applique la matrice de transition sur X_t pour pr√©dire X_t1.
        - Plus la MAE est basse, plus la matrice de transition est adapt√©e.

    Example:
        >>> score = evaluer_matrice_transition(M, X_t, X_t1)
        >>> logger.info(score)
        7.32

    √âtapes:
        1. Calcule M @ X_t pour obtenir les pr√©dictions.
        2. Calcule la MAE avec X_t1.

    Tips:
        - Peut servir de crit√®re objectif pour s√©lectionner le meilleur mod√®le.

    Utilisation:
        Appeler √† chaque test de matrice ou pour comparer diff√©rentes variantes
        de mod√®les.

    Limitation:
        - Les matrices doivent avoir la bonne dimension (coh√©rentes).

    See also:
        - calculer_mae
    """
    predictions = matrice_transition @ X_t
    return calculer_mae(predictions, X_t1)


@log_debut_fin_logger_dynamique("logger")
def chercher_meilleur_alpha_matrice_score(
        generateur_de_matrice, X_t: np.ndarray, X_t1: np.ndarray, 
        liste_alpha: list[float], logger: logging.Logger
        ) -> tuple[float, np.ndarray, float]:
    """
    Teste plusieurs valeurs d‚Äôalpha pour trouver la matrice de transition qui
    minimise la MAE.

    Args:
        generateur_de_matrice (callable): Fonction prenant alpha, renvoyant une 
            matrice de transition (ex‚ÄØ: pour LS ou EM).
        X_t (np.ndarray): Cas √† t (entr√©e mod√®le).
        X_t1 (np.ndarray): Cas √† t+1 (cible √† pr√©dire).
        liste_alpha (list[float]): Liste des valeurs d‚Äôalpha √† tester.
        logger (logging.Logger): Logger pour affichage et suivi de la progression.


    Returns:
        tuple: (meilleur_alpha, meilleure_matrice, meilleur_score)
            - meilleur_alpha (float): Valeur d‚Äôalpha donnant la plus faible MAE.
            - meilleure_matrice (np.ndarray): Matrice de transition optimale.
            - meilleur_score (float): Meilleure MAE obtenue.

    Note:
        - Affiche la MAE pour chaque alpha test√©.
        - Tr√®s utile pour l‚Äôoptimisation d‚Äôhyperparam√®tres par grille.

    Example:
        >>> a, M, score = chercher_meilleur_alpha_matrice_score(
                gen, X_t, X_t1, [0, 0.25, 0.5, 0.75, 1])
        >>> logger.info(a, score)

    √âtapes:
        1. Pour chaque alpha de la liste‚ÄØ:
            a. G√©n√©re la matrice de transition.
            b. √âvalue la MAE sur les donn√©es.
            c. Conserve la meilleure MAE et l‚Äôalpha associ√©.
        2. Retourne l‚Äôalpha, la matrice et le score optimaux.

    Tips:
        - Peut s‚Äôutiliser pour EM ou moindres carr√©s avec contraintes.

    Utilisation:
        √Ä appeler lors de la recherche du meilleur hyperparam√®tre alpha.

    Limitation:
        - Les performances peuvent d√©pendre de la granularit√© de la grille alpha.

    See also:
        - evaluer_matrice_transition
    """
    meilleur_score = float("inf")
    meilleur_alpha = None
    meilleure_matrice = None

    for alpha in liste_alpha:
        matrice = generateur_de_matrice(alpha)
        score = evaluer_matrice_transition(matrice, X_t, X_t1)
        logger.info(f"Alpha={alpha:.3f} | MAE={score:.4f}")
        if score < meilleur_score:
            meilleur_score = score
            meilleur_alpha = alpha
            meilleure_matrice = matrice

    return meilleur_alpha, meilleure_matrice, meilleur_score


@log_debut_fin_logger_dynamique("logger")
def generer_modeles_ponderes(
        matrice_moindre_carre: np.ndarray, matrice_esperance_maximisation: np.ndarray, 
        X_t: np.ndarray, X_t1: np.ndarray, emplacement_sauvegarde: str, 
        logger: logging.Logger, nombre_de_test: int = 400) -> dict[str, Any]:
    """
    Cr√©e et enregistre plusieurs mod√®les combin√©s pond√©r√©s (LS/EM), 
    puis s√©lectionne la meilleure combinaison selon la MAE.

    Args:
        matrice_moindre_carre (np.ndarray): Matrice de transition LS.
        matrice_esperance_maximisation (np.ndarray): Matrice de transition EM.
        X_t (np.ndarray): Matrice des cas √† t (entr√©e).
        X_t1 (np.ndarray): Matrice des cas √† t+1 (cible).
        emplacement_sauvegarde (str): Dossier o√π sauvegarder les mod√®les.
        logger (logging.Logger): Logger pour affichage et suivi de la progression.
        nombre_de_test (int): Nombre de combinaisons √† tester (d√©faut 400).

    Returns:
        dict[str, Any]: Dictionnaire d√©taillant la meilleure combinaison
            trouv√©e‚ÄØ: poids LS, poids EM, MAE, nom du fichier, etc.

    Note:
        - Sauvegarde chaque combinaison test√©e dans un fichier distinct,
          et enregistre la meilleure en r√©sum√©.
        - Les poids test√©s vont de 0% LS/100% EM √† 100% LS/0% EM.
        - Utilise tqdm pour afficher la progression de g√©n√©ration.

    Example:
        >>> res = generer_modeles_ponderes(M_LS, M_EM, X_t, X_t1, "dossier")
        >>> logger.info(res["poids_LS"], res["mae_validation"])

    √âtapes:
        1. Pour chaque pond√©ration (0 √† 1, par pas r√©gulier)‚ÄØ:
            a. Calcule la matrice pond√©r√©e.
            b. Pr√©dit X_t1 √† partir de X_t.
            c. Calcule la MAE et enregistre le mod√®le.
            d. M√©morise la meilleure combinaison.
        2. Sauvegarde la meilleure au format JSON.

    Tips:
        - Permet d‚Äôexplorer tous les m√©langes LS/EM, pas juste 0, 0.5, 1.
        - Peut consommer du temps/disque‚ÄØ: adapter nombre_de_test si besoin.

    Utilisation:
        √Ä appeler √† la fin de l‚Äôapprentissage pour s√©lectionner le
        meilleur mod√®le combin√© (souvent avant d√©ploiement/pr√©diction).

    Limitation:
        - La granularit√© d√©pend de nombre_de_test.
        - Peut g√©n√©rer beaucoup de fichiers si nombre_de_test est √©lev√©.

    See also:
        - fusionner_les_modeles (markov_models)
    """

    if not os.path.exists(emplacement_sauvegarde):
        os.makedirs(emplacement_sauvegarde)

    meilleur_mae = float("inf")
    meilleur_combinaison = None

    logger.info(f"D√©but de la g√©n√©ration de {nombre_de_test} mod√®les pond√©r√©s LS/EM‚Ä¶")
    for i in tqdm(range(nombre_de_test + 1), desc="üîÑ G√©n√©ration des mod√®les"):
        poids_moindre_carre = i / nombre_de_test
        poids_esperance_maximisation = 1.0 - poids_moindre_carre
        matrice_fusionnee = (
            poids_moindre_carre * matrice_moindre_carre +
            poids_esperance_maximisation * matrice_esperance_maximisation
        )
        preds = matrice_fusionnee @ X_t
        mae = np.mean(np.abs(X_t1 - preds))
        result = {
            "poids_LS": poids_moindre_carre,
            "poids_EM": poids_esperance_maximisation,
            "mae_validation": mae,
            "matrice_de_transition": matrice_fusionnee.tolist()
        }
        nom_fichier = (
            f"model_combination_idx{i:04d}"
            f"_ls{poids_moindre_carre:.4f}"
            f"_em{poids_esperance_maximisation:.4f}.json"
        )
        emplacement_fichier = os.path.join(emplacement_sauvegarde, nom_fichier)
        try:
            sauvegarder_json(result, emplacement_fichier, ecrasement = True,
                             logger = logger)
        except Exception as exception:
            logger.error(f"Erreur de sauvegarde du mod√®le {nom_fichier}: {exception}",
                         exc_info = True)

        if mae < meilleur_mae:
            meilleur_mae = mae
            meilleur_combinaison = result
            meilleur_combinaison["nom_du_fichier"] = nom_fichier

    meilleur_emplacement_json = os.path.join(
        emplacement_sauvegarde, "best_combination_model.json")   
    try:
        # Sauvegarder la meilleure combinaison de mod√®le
        sauvegarder_json(meilleur_combinaison, meilleur_emplacement_json, 
                         ecrasement = True, logger = logger)
    except Exception as exception:
        logger.error(f"Erreur de sauvegarde du meilleur mod√®le : {exception}", 
                     exc_info = True)

    logger.info(f"Fin de la g√©n√©ration. Meilleur MAE={meilleur_mae:.6f} "
                f"sauvegard√© dans {meilleur_emplacement_json}")

    logger.info(f"\nMeilleur mod√®le combin√© : {meilleur_combinaison['nom_du_fichier']} "
          f"avec MAE = {meilleur_mae:.6f}")
    logger.info(f"R√©sum√© sauvegard√© dans : {meilleur_emplacement_json}")

    return meilleur_combinaison


@log_debut_fin_logger_dynamique("logger")
def entrainer_modele_moindre_carre(
        X_t, X_t1, matrice_des_poids_geo, nbr_communes, logger: logging.Logger,
        valeurs_alpha_geo: list[float] = None) -> dict:
    """
    Entra√Æne un mod√®le moindres carr√©s g√©ographiquement contraint
    pour diff√©rents alpha et s√©lectionne le meilleur.

    Args:
        X_t (np.ndarray): Matrice des cas √† t.
        X_t1 (np.ndarray): Matrice des cas √† t+1.
        matrice_des_poids_geo (np.ndarray): Matrice des poids g√©ographiques.
        nbr_communes (int): Nombre de communes.
        logger (logging.Logger): Logger pour affichage et suivi de la progression.
        valeurs_alpha_geo (list[float], optionnel): Valeurs d‚Äôalpha √† tester.

    Returns:
        dict: Dictionnaire des r√©sultats pour chaque alpha, plus les m√©tadonn√©es
            (meilleur alpha, meilleure MAE, nombre d‚Äôobservations).

    Note:
        - Pour chaque alpha, g√©n√®re une matrice contrainte, la normalise,
          et √©value sa MAE.
        - Enregistre la meilleure configuration selon la MAE.

    Example:
        >>> d = entrainer_modele_moindre_carre(X_t, X_t1, M_geo, 19, [0, 0.5, 1])
        >>> logger.info(d["metadata"]["meilleur_alpha_geo"])

    √âtapes:
        1. Estime la matrice de base (LS).
        2. Pour chaque alpha, applique les contraintes et √©value la MAE.
        3. R√©cup√®re et retourne la meilleure configuration.

    Tips:
        - Adapte la grille d‚Äôalpha selon le besoin de pr√©cision.

    Utilisation:
        √Ä utiliser dans tout pipeline Markov pour la validation des mod√®les
        moindres carr√©s avec contraintes.

    Limitation:
        - Les r√©sultats d√©pendent fortement de la coh√©rence des donn√©es et des poids.

    See also:
        - appliquer_des_contraintes_geographiques
        - forcer_valeurs_positives_et_normaliser
    """
    if valeurs_alpha_geo is None:
        valeurs_alpha_geo = generer_les_valeurs_alpha(0.025)
    # Estimation de la matrice de base
    matrice_de_base = estimer_matrice_de_transition_par_moindre_carre(
        X_t, X_t1, nbr_communes, logger)
    # Test de diff√©rents alpha_geo
    modeles = {}
    meilleur_alpha = None
    meilleur_taux_d_erreurs = float("inf")
    for valeur_alpha_geo in valeurs_alpha_geo:
        logger.info(f"\n Test alpha_geo = {valeur_alpha_geo}")
        # Application des contraintes g√©ographiques
        matrice_avec_contraintes = appliquer_des_contraintes_geographiques(
            matrice_de_base, matrice_des_poids_geo, nbr_communes, logger,
            valeur_alpha_geo)
        # Post-traitement : forcer les valeurs positives + normaliser
        matrice_avec_contraintes = forcer_valeurs_positives_et_normaliser(
            matrice_avec_contraintes, logger)
        # √âvaluation
        mae = np.mean(np.abs((X_t1 - matrice_avec_contraintes @ X_t)))
        modeles[f"{ALPHA_GEO}{valeur_alpha_geo}"] = {
            ALPHA_GEO : valeur_alpha_geo,
            TRANSITION_MATRIX : matrice_avec_contraintes.tolist(),
            MAE_VALIDATION: mae
        }
        logger.info(f"MAE validation : {mae:.2f}")
        if mae < meilleur_taux_d_erreurs:
            meilleur_taux_d_erreurs = mae
            meilleur_alpha = valeur_alpha_geo
    logger.info(f"\n Meilleur mod√®le : alpha_geo = {meilleur_alpha} \n "
          f"(MAE = {meilleur_taux_d_erreurs:.2f})")
    # M√©tadonn√©es
    modeles[METADATA] = {
        BEST_ALPHA_GEO: meilleur_alpha,
        BEST_MAE: meilleur_taux_d_erreurs,
        NBR_OBSERVATION: X_t.shape[1]
    }
    return modeles


@log_debut_fin_logger_dynamique("logger")
def entrainer_le_modele_esperance_maximisation(
        X_t: np.ndarray, X_t1: np.ndarray, matrice_des_poids_geo: np.ndarray, 
        nbr_communes: int, logger: logging.Logger, 
        valeurs_alpha_geo: list[float] = None) -> dict[str, dict]:
    """
    Entra√Æne un mod√®le par EM (avec contraintes g√©ographiques) pour
    diff√©rentes valeurs d‚Äôalpha et s√©lectionne le meilleur mod√®le.

    Args:
        X_t (np.ndarray): Cas √† t.
        X_t1 (np.ndarray): Cas √† t+1.
        matrice_des_poids_geo (np.ndarray): Matrice des poids g√©ographiques.
        nbr_communes (int): Nombre de communes.
        logger (logging.Logger): Logger pour affichage et suivi de la progression.
        valeurs_alpha_geo (list[float]): Liste d‚Äôalpha √† tester.

    Returns:
        dict[str, dict]: R√©sultats d√©taill√©s pour chaque alpha, plus les
            m√©tadonn√©es (meilleur alpha, meilleure MAE, n observations).

    Note:
        - Pour chaque alpha, applique EM avec contrainte g√©o, puis √©value la MAE.
        - Affiche la progression et la performance pour chaque alpha.

    Example:
        >>> res = entrainer_le_modele_esperance_maximisation(
                X_t, X_t1, M_geo, 19, [0.0, 0.5, 1.0])
        >>> logger.info(res["metadata"]["meilleur_alpha_geo"])

    √âtapes:
        1. Pour chaque alpha‚ÄØ: entra√Ænement EM puis √©valuation.
        2. S√©lectionne le mod√®le optimal et sauvegarde les m√©triques.

    Tips:
        - Le processus peut √™tre lent pour de grandes matrices ou
          beaucoup d‚Äôalpha.

    Utilisation:
        √Ä utiliser pour toute comparaison entre m√©thodes LS et EM,
        ou lors de la validation crois√©e.

    Limitation:
        - Peut ne pas converger pour certaines configurations extr√™mes.
        - Le choix de la grille alpha influe sur la performance.

    See also:
        - esperance_maximisation_markov_avec_geo (markov_models)
    """
    if valeurs_alpha_geo is None:
        valeurs_alpha_geo = generer_les_valeurs_alpha( 1 / NOMBRE_DE_TESTS_SOUHAITES)    
    modeles = {}
    meilleur_alpha = None
    meilleur_mae = float('inf')
    for alpha_geo in valeurs_alpha_geo:
        logger.info(f"\nEM avec alpha_geo={alpha_geo}:.2f")
        matrice_esperance_maximisation = esperance_maximisation_markov_avec_geo(
            X_t, X_t1, matrice_des_poids_geo, logger, alpha_geo = alpha_geo)
        # √âvaluer
        preds = matrice_esperance_maximisation @ X_t
        mae = np.mean(np.abs(X_t1 - preds))

        modeles[f"{ALPHA_GEO}{alpha_geo}"] = {
            ALPHA_GEO: alpha_geo,
            TRANSITION_MATRIX: matrice_esperance_maximisation.tolist(),
            MAE_VALIDATION: mae
        }
        logger.info(f"MAE validation : {mae:.4f}")
        if mae < meilleur_mae:
            meilleur_mae = mae
            meilleur_alpha = alpha_geo
    modeles[METADATA] = {
        BEST_ALPHA_GEO: meilleur_alpha,
        BEST_MAE: meilleur_mae,
        NBR_OBSERVATION: X_t.shape[1]
    }
    return modeles


@log_debut_fin_logger_dynamique("logger")
def generer_les_valeurs_alpha(incrementation:float = 1 / NOMBRE_DE_TESTS_SOUHAITES):
    """
    G√©n√®re une liste r√©guli√®re de valeurs alpha de 0 √† 1 (inclus), par pas donn√©.

    Args:
        incrementation (float): Pas entre chaque alpha (par d√©faut 1/N).

    Returns:
        list[float]: Liste de tous les alpha g√©n√©r√©s (0.0, 0.025, ..., 1.0).

    Note:
        - S‚Äôassure que la valeur 1.0 est pr√©sente, m√™me si le pas ne tombe pas
          pile dessus.
        - Arrondit √† 3 d√©cimales pour la stabilit√© num√©rique et la lisibilit√©.

    Example:
        >>> generer_les_valeurs_alpha(0.25)
        [0.0, 0.25, 0.5, 0.75, 1.0]

    √âtapes:
        1. Cr√©e la liste via np.arange.
        2. Arrondit chaque valeur √† 3 d√©cimales.
        3. Ajoute 1.0 si absent.

    Tips:
        - Adapter l‚Äôincr√©ment selon la r√©solution d‚Äôhyperparam√®tre souhait√©e.

    Utilisation:
        Pour toute boucle sur alpha en tuning de mod√®le ou grid search.

    Limitation:
        - Peut g√©n√©rer des doublons si l‚Äôincr√©ment tombe pile sur 1.0.

    See also:
        - entrainer_le_modele_esperance_maximisation
        - entrainer_modele_moindre_carre
    """
    valeurs_alpha = [round(a, 3) for a in np.arange(
        0.0, 1.0 + incrementation, incrementation)
    ]
    if valeurs_alpha[-1] != 1.0:
        valeurs_alpha.append(1.0)
    return valeurs_alpha